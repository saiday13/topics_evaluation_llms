# Topic Model Evaluation with Large Language Models
The aim of this project is to evaluate the correlation between the automated topic model and large language models (LLMs) evaluations. Following the work of Hoyle et al. [Is Automated Topic Model Evaluation Broken?:
The Incoherence of Coherence](https://arxiv.org/pdf/2107.02173), we replace the human evaluation with LLMs evaluation in two tasks: intrusion detection and rating.
